{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /Users/marina/Documents/GitHub/NLP_HW1/dict.txt.big ...\n",
      "Loading model from cache /var/folders/wf/26_5_j6173n3fyjcdd29_r3r0000gn/T/jieba.u679c42d08b25284bf1a3bc8ecfdc9e37.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['烏雲在我們心裡擱下一塊陰影', '我聆聽沉寂已久的心情', '清晰透明 就像美麗的風景', '總在回憶裡才看的清', '被傷透的心能不能夠繼續愛我', '我用力牽起沒溫度的雙手', '過往溫柔已經被時間上鎖', '只剩揮散不去的難過', '緩緩飄落的楓葉像思念', '我點燃燭火溫暖歲末的秋天', '極光掠奪天邊 北風掠過想你的容顏', '我把愛燒成了落葉', '卻換不回熟悉的那張臉', '緩緩飄落的楓葉像思念', '為何挽回要趕在冬天來之前', '愛你穿越時間 兩行來自秋末的眼淚', '讓愛滲透了地面', '我要的只是你在我身邊', '被傷透的心能不能夠繼續愛我', '我用力牽起沒溫度的雙手', '過往溫柔已經被時間上鎖', '只剩揮散不去的難過', '在山腰間飄逸的紅雨', '隨著北風凋零 我輕輕搖曳風鈴', '想 喚醒被遺棄的愛情', '雪花已鋪滿了地 深怕窗外楓葉已結成冰', '緩緩飄落的楓葉像思念', '我點燃燭火溫暖歲末的秋天', '極光掠奪天邊 北風掠過想你的容顏', '我把愛燒成了落葉', '卻換不回熟悉的那張臉', '緩緩飄落的楓葉像思念', '為何挽回要趕在冬天來之前', '愛你穿越時間 兩行來自秋末的眼淚', '讓愛滲透了地面', '我要的只是你在我身邊']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.172 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['烏雲', '擱下', '一塊', '陰影'], ['聆聽', '沉寂已久', '心情'], ['清晰', '透明', '美麗', '風景'], ['總在', '回憶', '裡才', '清'], ['傷透', '心', '能不能夠', '愛我'], ['用力', '牽起', '溫度', '雙手'], ['過往', '溫柔', '上鎖'], ['剩揮散', '難過'], ['緩緩', '飄落', '楓葉', '思念'], ['點燃', '燭火', '溫暖', '歲末', '秋天'], ['極光', '掠奪', '天邊', '北風', '掠過', '想', '容顏'], ['愛', '燒成', '落葉'], ['卻換', '不回', '熟悉', '張臉'], ['緩緩', '飄落', '楓葉', '思念'], ['挽回', '冬天'], ['愛你', '穿越', '兩行', '秋末', '眼淚'], ['愛', '滲透', '地面'], ['我要', '身邊'], ['傷透', '心', '能不能夠', '愛我'], ['用力', '牽起', '溫度', '雙手'], ['過往', '溫柔', '上鎖'], ['剩揮散', '難過'], ['山', '腰間', '飄逸', '紅雨'], ['北風', '凋零', '輕輕', '搖曳', '風鈴'], ['想', '喚醒', '被遺棄', '愛情'], ['雪花', '鋪滿', '深怕', '窗外', '楓葉', '結成', '冰'], ['緩緩', '飄落', '楓葉', '思念'], ['點燃', '燭火', '溫暖', '歲末', '秋天'], ['極光', '掠奪', '天邊', '北風', '掠過', '想', '容顏'], ['愛', '燒成', '落葉'], ['卻換', '不回', '熟悉', '張臉'], ['緩緩', '飄落', '楓葉', '思念'], ['挽回', '冬天'], ['愛你', '穿越', '兩行', '秋末', '眼淚'], ['愛', '滲透', '地面'], ['我要', '身邊']]\n"
     ]
    }
   ],
   "source": [
    "with open(\"dataset.txt\", \"r\") as fd:\n",
    "    data = fd.read().splitlines()\n",
    "print(data)\n",
    "jieba.set_dictionary('dict.txt.big')\n",
    "stopwords = [line.strip() for line in open(\"stop_words.txt\",encoding=\"utf-8\").readlines()]  \n",
    "\n",
    "size = len(data)\n",
    "\n",
    "new_list = []\n",
    "for i in range(size):\n",
    "    new_list.append([])\n",
    "    \n",
    "for i in range(size):\n",
    "    new_list[i] = jieba.lcut(data[i])\n",
    "\n",
    "#print(new_list)\n",
    "word = []\n",
    "for i in range(size):\n",
    "    word.append([])\n",
    "for i in range(size):\n",
    "    for seg in new_list[i]:\n",
    "        if seg not in stopwords:\n",
    "            if seg == \" \":  \n",
    "                continue  \n",
    "            else:  \n",
    "                word[i].append(seg)\n",
    "\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count = []\n",
    "for file in word:\n",
    "    count = {}\n",
    "    for w in file:\n",
    "        if w in count:\n",
    "            count[w] += 1\n",
    "        else:\n",
    "            count[w] = 1\n",
    "    words_count.append(count)\n",
    "    \n",
    "#print(words_count)\n",
    "\n",
    "#計算tf\n",
    "words_frequency = []\n",
    "for word_count in words_count:\n",
    "    all_count = sum(word_count.values())\n",
    "    fre = {}\n",
    "    for word,count in word_count.items():\n",
    "        fre[word] = round(count/all_count,4)\n",
    "    words_frequency.append(fre)\n",
    "#print(words_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('楓葉', 5), ('緩緩', 4), ('飄落', 4), ('思念', 4), ('愛', 4), ('北風', 3), ('想', 3), ('傷透', 2), ('心', 2), ('能不能夠', 2), ('愛我', 2), ('用力', 2), ('牽起', 2), ('溫度', 2), ('雙手', 2), ('過往', 2), ('溫柔', 2), ('上鎖', 2), ('剩揮散', 2), ('難過', 2), ('點燃', 2), ('燭火', 2), ('溫暖', 2), ('歲末', 2), ('秋天', 2), ('極光', 2), ('掠奪', 2), ('天邊', 2), ('掠過', 2), ('容顏', 2), ('燒成', 2), ('落葉', 2), ('卻換', 2), ('不回', 2), ('熟悉', 2), ('張臉', 2), ('挽回', 2), ('冬天', 2), ('愛你', 2), ('穿越', 2), ('兩行', 2), ('秋末', 2), ('眼淚', 2), ('滲透', 2), ('地面', 2), ('我要', 2), ('身邊', 2), ('烏雲', 1), ('擱下', 1), ('一塊', 1), ('陰影', 1), ('聆聽', 1), ('沉寂已久', 1), ('心情', 1), ('清晰', 1), ('透明', 1), ('美麗', 1), ('風景', 1), ('總在', 1), ('回憶', 1), ('裡才', 1), ('清', 1), ('山', 1), ('腰間', 1), ('飄逸', 1), ('紅雨', 1), ('凋零', 1), ('輕輕', 1), ('搖曳', 1), ('風鈴', 1), ('喚醒', 1), ('被遺棄', 1), ('愛情', 1), ('雪花', 1), ('鋪滿', 1), ('深怕', 1), ('窗外', 1), ('結成', 1), ('冰', 1)]\n"
     ]
    }
   ],
   "source": [
    "#idf\n",
    "all_words = []\n",
    "for words in words_count:\n",
    "    all_words.extend(words.keys())\n",
    "    \n",
    "occurrences_of_word = {}\n",
    "for word in all_words:\n",
    "    if word in occurrences_of_word:\n",
    "        occurrences_of_word[word] += 1\n",
    "    else:\n",
    "        occurrences_of_word[word] = 1\n",
    "        \n",
    "print(sorted(occurrences_of_word.items(), key=lambda x:x[1],reverse=True))\n",
    "        \n",
    "inverse_document_frequency = []     \n",
    "for word_count in words_count:\n",
    "    invFre = {} \n",
    "    for word in word_count.keys():\n",
    "        occurrences = occurrences_of_word[word]\n",
    "        invFre[word] = math.log(round((len(words_count)/occurrences),4))\n",
    "    inverse_document_frequency.append(invFre)\n",
    "    \n",
    "#print(inverse_document_frequency)\n",
    "#print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tf_idf = []\n",
    "for i,words in enumerate(words_frequency):\n",
    "    tf_idf = {}\n",
    "    for word,freq in words.items():\n",
    "        tf_idf[word] = freq*inverse_document_frequency[i][word]\n",
    "    all_tf_idf.append(tf_idf)\n",
    "\n",
    "#print(all_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-253-8c86114abf63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moccurrences_of_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moccurrences_of_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'center'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moccurrences_of_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moccurrences_of_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.bar(range(len(occurrences_of_word)), list(occurrences_of_word.values()), align='center')\n",
    "plt.xticks(range(len(occurrences_of_word)), list(occurrences_of_word.keys()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
